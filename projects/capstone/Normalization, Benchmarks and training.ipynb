{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "# import the relevant Keras modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "import keras\n",
    "print (keras.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [12, 8] # width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "\n",
    "# Data sources\n",
    "include_bitcoin = 1\n",
    "include_ethereum = 1\n",
    "include_litecoin = 1\n",
    "include_ripple = 1\n",
    "include_goog_trends = 1\n",
    "include_stocktwits = 0\n",
    "\n",
    "# Data duration (training + test)\n",
    "start_date_ = '2015-08-07'\n",
    "end_date_ = '2018-04-18'\n",
    "\n",
    "start_date = start_date_.replace(\"-\", \"\")\n",
    "end_date = end_date_.replace(\"-\", \"\")\n",
    "\n",
    "# split_date = '2018-02-02'\n",
    "split_date = '2018-01-20'  # >10% of full dataset\n",
    "\n",
    "# Target : Predict bitcoin (OR ethereum) daily returns OR volatility \n",
    "# target = 'btc_daily_ret'\n",
    "# target = 'btc_volatility'\n",
    "\n",
    "target = 'eth_daily_ret'\n",
    "# target = 'eth_volatility'\n",
    "\n",
    "# Algorithm (LSTM - tune with GridSearch and sklearn's TimeSeriesSplit)\n",
    "# window_size = 10\n",
    "# how many days in advance\n",
    "\n",
    "coins = ['bitcoin', 'ethereum', 'litecoin', 'ripple']\n",
    "coin_symbol = {'bitcoin': 'btc', 'ethereum': 'eth', 'litecoin': 'ltc', 'ripple': 'xrp'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_df = pd.read_pickle(\"data/full_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>btc_Close</th>\n",
       "      <th>btc_Volume</th>\n",
       "      <th>eth_Close</th>\n",
       "      <th>eth_Volume</th>\n",
       "      <th>ltc_Close</th>\n",
       "      <th>ltc_Volume</th>\n",
       "      <th>xrp_Close</th>\n",
       "      <th>xrp_Volume</th>\n",
       "      <th>btc_close_off_high</th>\n",
       "      <th>...</th>\n",
       "      <th>ltc_30day_ret</th>\n",
       "      <th>xrp_close_off_high</th>\n",
       "      <th>xrp_volatility</th>\n",
       "      <th>xrp_daily_ret</th>\n",
       "      <th>xrp_7day_ret</th>\n",
       "      <th>xrp_30day_ret</th>\n",
       "      <th>btc_trends</th>\n",
       "      <th>eth_trends</th>\n",
       "      <th>ltc_trends</th>\n",
       "      <th>xrp_trends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-08-07</td>\n",
       "      <td>279.58</td>\n",
       "      <td>42484800</td>\n",
       "      <td>2.770000</td>\n",
       "      <td>164329</td>\n",
       "      <td>4.21</td>\n",
       "      <td>4192810</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>363643</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036946</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>0.020449</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>30.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>261.00</td>\n",
       "      <td>58533000</td>\n",
       "      <td>0.753325</td>\n",
       "      <td>674188</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4917730</td>\n",
       "      <td>0.008476</td>\n",
       "      <td>678295</td>\n",
       "      <td>-0.969823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087678</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.066634</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-08-09</td>\n",
       "      <td>265.08</td>\n",
       "      <td>23789600</td>\n",
       "      <td>0.701897</td>\n",
       "      <td>532170</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3064680</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>531969</td>\n",
       "      <td>0.411945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.041372</td>\n",
       "      <td>0.038190</td>\n",
       "      <td>0.038190</td>\n",
       "      <td>0.038190</td>\n",
       "      <td>30.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>264.47</td>\n",
       "      <td>20979400</td>\n",
       "      <td>0.708448</td>\n",
       "      <td>405283</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2239890</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>472973</td>\n",
       "      <td>-0.155756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>-0.949686</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>-0.007036</td>\n",
       "      <td>-0.007036</td>\n",
       "      <td>-0.007036</td>\n",
       "      <td>30.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>270.39</td>\n",
       "      <td>25433900</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>1463100</td>\n",
       "      <td>4.16</td>\n",
       "      <td>3426300</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>282461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053165</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>31.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  btc_Close  btc_Volume  eth_Close  eth_Volume  ltc_Close  \\\n",
       "0 2015-08-07     279.58    42484800   2.770000      164329       4.21   \n",
       "1 2015-08-08     261.00    58533000   0.753325      674188       3.85   \n",
       "2 2015-08-09     265.08    23789600   0.701897      532170       3.90   \n",
       "3 2015-08-10     264.47    20979400   0.708448      405283       3.95   \n",
       "4 2015-08-11     270.39    25433900   1.070000     1463100       4.16   \n",
       "\n",
       "   ltc_Volume  xrp_Close  xrp_Volume  btc_close_off_high     ...      \\\n",
       "0     4192810   0.008152      363643            0.597015     ...       \n",
       "1     4917730   0.008476      678295           -0.969823     ...       \n",
       "2     3064680   0.008808      531969            0.411945     ...       \n",
       "3     2239890   0.008750      472973           -0.155756     ...       \n",
       "4     3426300   0.008591      282461            1.000000     ...       \n",
       "\n",
       "   ltc_30day_ret  xrp_close_off_high  xrp_volatility  xrp_daily_ret  \\\n",
       "0       0.036946            0.987805        0.020449       0.016459   \n",
       "1      -0.087678            0.147059        0.066634       0.038217   \n",
       "2       0.015625            0.914530        0.041372       0.038190   \n",
       "3       0.012821           -0.949686        0.018044      -0.007036   \n",
       "4       0.053165           -1.000000        0.019998      -0.018284   \n",
       "\n",
       "   xrp_7day_ret  xrp_30day_ret  btc_trends  eth_trends  ltc_trends  xrp_trends  \n",
       "0      0.016459       0.016459        30.0        65.0        68.0        77.0  \n",
       "1      0.038217       0.038217        29.0        60.0        66.0        80.0  \n",
       "2      0.038190       0.038190        30.0        48.0        55.0        72.0  \n",
       "3     -0.007036      -0.007036        30.0        46.0        51.0        85.0  \n",
       "4     -0.018284      -0.018284        31.0        48.0        64.0        77.0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1fbe5f68860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fbe5fb3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(np.log(dataset_df['eth_volatility']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1fbe5f68940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fbe6036d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(np.log(dataset_df['btc_volatility']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "Benchmark -I: Price-Persistent model, assumes the close price is same as the previous day. Implies that the estimated daily returns and volatility are both zero\n",
    "\n",
    "Benchmark - II: Market-Persistent model, assumes the daily returns is same as the previous day. Implies the estimated daily returns and volatility are same as the previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting: eth_daily_ret\n",
      "Benchmark-I: MAE 0.04616467948128482\n",
      "Benchmark-I: MSE 0.003542452975106516\n"
     ]
    }
   ],
   "source": [
    "## Price-PERSISTENT MODEL: Benchmark-I\n",
    "\n",
    "print (\"Predicting:\", target)\n",
    "\n",
    "# Evaluate on test dataset\n",
    "# no change from previous day => pred_bitcoin = 0 (percent change) for persistence model\n",
    "actual_target = dataset_df[dataset_df['Date']>= split_date][target].values\n",
    "# pred_target = 0\n",
    "\n",
    "bm1_mae = np.mean(np.absolute((actual_target)))\n",
    "bm1_mse = np.mean(np.square((actual_target)))\n",
    "\n",
    "print(\"Benchmark-I: MAE\", bm1_mae)\n",
    "print(\"Benchmark-I: MSE\", bm1_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark-II: MAE 0.06758115531787437\n",
      "Benchmark-II: MSE 0.007433466612293294\n"
     ]
    }
   ],
   "source": [
    "## Market-PERSISTENT MODEL: Benchmark-II\n",
    "\n",
    "# Evaluate on test dataset\n",
    "# same market behavior as previous day\n",
    "pred_target = dataset_df[dataset_df['Date']>= datetime.datetime.strptime(split_date, '%Y-%m-%d') - \n",
    "                      datetime.timedelta(days=1)][target][:-1].values\n",
    "\n",
    "bm2_mae = np.mean(np.absolute((actual_target-pred_target)))\n",
    "bm2_mse = np.mean(np.square((actual_target-pred_target)))\n",
    "\n",
    "print(\"Benchmark-II: MAE\", bm2_mae)\n",
    "print(\"Benchmark-II: MSE\", bm2_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>btc_Close</th>\n",
       "      <th>btc_Volume</th>\n",
       "      <th>eth_Close</th>\n",
       "      <th>eth_Volume</th>\n",
       "      <th>ltc_Close</th>\n",
       "      <th>ltc_Volume</th>\n",
       "      <th>xrp_Close</th>\n",
       "      <th>xrp_Volume</th>\n",
       "      <th>btc_close_off_high</th>\n",
       "      <th>...</th>\n",
       "      <th>ltc_30day_ret</th>\n",
       "      <th>xrp_close_off_high</th>\n",
       "      <th>xrp_volatility</th>\n",
       "      <th>xrp_daily_ret</th>\n",
       "      <th>xrp_7day_ret</th>\n",
       "      <th>xrp_30day_ret</th>\n",
       "      <th>btc_trends</th>\n",
       "      <th>eth_trends</th>\n",
       "      <th>ltc_trends</th>\n",
       "      <th>xrp_trends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-08-07</td>\n",
       "      <td>279.58</td>\n",
       "      <td>42484800</td>\n",
       "      <td>2.770000</td>\n",
       "      <td>164329</td>\n",
       "      <td>4.21</td>\n",
       "      <td>4192810</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>363643</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036946</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>0.020449</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>0.016459</td>\n",
       "      <td>30.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>261.00</td>\n",
       "      <td>58533000</td>\n",
       "      <td>0.753325</td>\n",
       "      <td>674188</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4917730</td>\n",
       "      <td>0.008476</td>\n",
       "      <td>678295</td>\n",
       "      <td>-0.969823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087678</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.066634</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.038217</td>\n",
       "      <td>29.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-08-09</td>\n",
       "      <td>265.08</td>\n",
       "      <td>23789600</td>\n",
       "      <td>0.701897</td>\n",
       "      <td>532170</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3064680</td>\n",
       "      <td>0.008808</td>\n",
       "      <td>531969</td>\n",
       "      <td>0.411945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.914530</td>\n",
       "      <td>0.041372</td>\n",
       "      <td>0.038190</td>\n",
       "      <td>0.038190</td>\n",
       "      <td>0.038190</td>\n",
       "      <td>30.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>264.47</td>\n",
       "      <td>20979400</td>\n",
       "      <td>0.708448</td>\n",
       "      <td>405283</td>\n",
       "      <td>3.95</td>\n",
       "      <td>2239890</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>472973</td>\n",
       "      <td>-0.155756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>-0.949686</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>-0.007036</td>\n",
       "      <td>-0.007036</td>\n",
       "      <td>-0.007036</td>\n",
       "      <td>30.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>270.39</td>\n",
       "      <td>25433900</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>1463100</td>\n",
       "      <td>4.16</td>\n",
       "      <td>3426300</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>282461</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053165</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>31.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  btc_Close  btc_Volume  eth_Close  eth_Volume  ltc_Close  \\\n",
       "0 2015-08-07     279.58    42484800   2.770000      164329       4.21   \n",
       "1 2015-08-08     261.00    58533000   0.753325      674188       3.85   \n",
       "2 2015-08-09     265.08    23789600   0.701897      532170       3.90   \n",
       "3 2015-08-10     264.47    20979400   0.708448      405283       3.95   \n",
       "4 2015-08-11     270.39    25433900   1.070000     1463100       4.16   \n",
       "\n",
       "   ltc_Volume  xrp_Close  xrp_Volume  btc_close_off_high     ...      \\\n",
       "0     4192810   0.008152      363643            0.597015     ...       \n",
       "1     4917730   0.008476      678295           -0.969823     ...       \n",
       "2     3064680   0.008808      531969            0.411945     ...       \n",
       "3     2239890   0.008750      472973           -0.155756     ...       \n",
       "4     3426300   0.008591      282461            1.000000     ...       \n",
       "\n",
       "   ltc_30day_ret  xrp_close_off_high  xrp_volatility  xrp_daily_ret  \\\n",
       "0       0.036946            0.987805        0.020449       0.016459   \n",
       "1      -0.087678            0.147059        0.066634       0.038217   \n",
       "2       0.015625            0.914530        0.041372       0.038190   \n",
       "3       0.012821           -0.949686        0.018044      -0.007036   \n",
       "4       0.053165           -1.000000        0.019998      -0.018284   \n",
       "\n",
       "   xrp_7day_ret  xrp_30day_ret  btc_trends  eth_trends  ltc_trends  xrp_trends  \n",
       "0      0.016459       0.016459        30.0        65.0        68.0        77.0  \n",
       "1      0.038217       0.038217        29.0        60.0        66.0        80.0  \n",
       "2      0.038190       0.038190        30.0        48.0        55.0        72.0  \n",
       "3     -0.007036      -0.007036        30.0        46.0        51.0        85.0  \n",
       "4     -0.018284      -0.018284        31.0        48.0        64.0        77.0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LSTM Begins\n",
    "symbols_ = [symbol+\"_\" for symbol in list(coin_symbol.values())]\n",
    "\n",
    "# model_data = dataset_df[['Date']+[symbol+metric for symbol in symbols_ \n",
    "#                                    for metric in ['Close', 'Volume', 'close_off_high', 'volatility', 'daily_ret']]\n",
    "#                         +[coin_symbol[coin]+\"_trends\" for coin in coins]]\n",
    "\n",
    "model_data = dataset_df\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_set, test_set = model_data[model_data['Date']<split_date], model_data[model_data['Date']>=split_date]\n",
    "\n",
    "# we don't need the date columns anymore\n",
    "########### DONT'WE ?? - If we are to cross-validation, we need the dates!! or we just use the index?\n",
    "training_set = training_set.drop('Date', 1)\n",
    "test_set = test_set.drop('Date', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['btc_Close',\n",
       "  'btc_Volume',\n",
       "  'eth_Close',\n",
       "  'eth_Volume',\n",
       "  'ltc_Close',\n",
       "  'ltc_Volume',\n",
       "  'xrp_Close',\n",
       "  'xrp_Volume',\n",
       "  'btc_trends',\n",
       "  'eth_trends',\n",
       "  'ltc_trends',\n",
       "  'xrp_trends']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM Parameters\n",
    "\n",
    "window_len = 7 # 25 # 20 10\n",
    "neurons = 100\n",
    "batch_size = 1\n",
    "epochs = 20\n",
    "no_of_features = training_set.shape[1]\n",
    "\n",
    "# Columns to be normalized - Instead of traditional normalization aschemes (like MinMax on training_set), \n",
    "# normalize the columns w.r.t to the first element of the window s.t the normalized first element would be 0 \n",
    "# Reference: \n",
    "norm_cols = [[symbol+metric for symbol in symbols_ for metric in ['Close','Volume']]          \n",
    "             +[coin_symbol[coin]+\"_trends\" for coin in coins]]\n",
    "\n",
    "norm_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_training_inputs = []\n",
    "for i in range(len(training_set)-window_len):\n",
    "    temp_set = training_set[i:(i+window_len)].copy()\n",
    "    for col in norm_cols:\n",
    "        temp_set.loc[:, col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
    "    LSTM_training_inputs.append(temp_set)\n",
    "\n",
    "# model output is next price normalised to 10th previous closing price\n",
    "LSTM_training_outputs = training_set[target][window_len:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "897\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "LSTM_test_inputs = []\n",
    "for i in range(len(test_set)-window_len):\n",
    "    temp_set = test_set[i:(i+window_len)].copy()\n",
    "    for col in norm_cols:\n",
    "        temp_set.loc[:, col] = temp_set[col]/temp_set[col].iloc[0] - 1\n",
    "    LSTM_test_inputs.append(temp_set)\n",
    "\n",
    "LSTM_test_outputs = test_set[target][window_len:].values\n",
    "\n",
    "print(len(training_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ???  easier to work with numpy arrays rather than pandas dataframes\n",
    "# especially as we now only have numerical data\n",
    "LSTM_training_inputs = [np.array(LSTM_training_input) for LSTM_training_input in LSTM_training_inputs]\n",
    "LSTM_training_inputs = np.array(LSTM_training_inputs)\n",
    "\n",
    "LSTM_test_inputs = [np.array(LSTM_test_inputs) for LSTM_test_inputs in LSTM_test_inputs]\n",
    "LSTM_test_inputs = np.array(LSTM_test_inputs)\n",
    "\n",
    "type(LSTM_test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasRegressor\n",
    "def create_model(dropout, neurons):\n",
    "    # use default values\n",
    "    init_mode='uniform'\n",
    "    weight_constraint=0 # or  4\n",
    "    lr = 0.01\n",
    "    momemntum=0\n",
    "\n",
    "    loss= 'mse'\n",
    "    optimizer='adam'\n",
    "    activ_func='linear'\n",
    "    output_size = 1 # TODO: Can we change this to 2 or even 4 (to predict all 4 at a time - {eth, btc}_{volatility, daily_ret})\n",
    "    \n",
    "    print (\"Start model: \", dropout, neurons)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(neurons[0], return_sequences=True, input_shape=(window_len, no_of_features)))\n",
    "    # model.add(LSTM(neurons, return_sequences=True, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(LSTM(neurons[1], return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(LSTM(neurons[2]))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    print (\"End compilation of model: \", dropout, neurons)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape:  (890, 7, 32)\n",
      "Train output shape:  (890,)\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] dropout=0.5, neurons=[10, 8, 4] .................................\n",
      "Start model:  0.5 [10, 8, 4]\n",
      "End compilation of model:  0.5 [10, 8, 4]\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 3s 20ms/step - loss: 0.0115\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0080\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0071\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0063\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0064\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0063\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0054\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "148/148 [==============================] - 0s 2ms/step\n",
      "150/150 [==============================] - 0s 547us/step\n",
      "[CV]  dropout=0.5, neurons=[10, 8, 4], score=-0.008283815530720292, total=  11.9s\n",
      "[CV] dropout=0.5, neurons=[10, 8, 4] .................................\n",
      "Start model:  0.5 [10, 8, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End compilation of model:  0.5 [10, 8, 4]\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 4s 12ms/step - loss: 0.0164\n",
      "Epoch 2/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0112\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0088\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0085\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0082\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0076\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0078\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0073\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0072\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0072\n",
      "148/148 [==============================] - 0s 3ms/step\n",
      "298/298 [==============================] - 0s 565us/step\n",
      "[CV]  dropout=0.5, neurons=[10, 8, 4], score=-0.003381627316058085, total=  15.6s\n",
      "[CV] dropout=0.5, neurons=[10, 8, 4] .................................\n",
      "Start model:  0.5 [10, 8, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   27.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End compilation of model:  0.5 [10, 8, 4]\n",
      "Epoch 1/10\n",
      "446/446 [==============================] - 4s 9ms/step - loss: 0.0062\n",
      "Epoch 2/10\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0060\n",
      "Epoch 3/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0058\n",
      "Epoch 4/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0058\n",
      "Epoch 5/10\n",
      "446/446 [==============================] - 2s 4ms/step - loss: 0.0058\n",
      "Epoch 6/10\n",
      "446/446 [==============================] - 2s 4ms/step - loss: 0.0057\n",
      "Epoch 7/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0057\n",
      "Epoch 8/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0056\n",
      "Epoch 9/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0056\n",
      "Epoch 10/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0056\n",
      "148/148 [==============================] - 0s 3ms/step\n",
      "446/446 [==============================] - 0s 598us/step\n",
      "[CV]  dropout=0.5, neurons=[10, 8, 4], score=-0.004614245095974577, total=  20.1s\n",
      "[CV] dropout=0.5, neurons=[10, 8, 4] .................................\n",
      "Start model:  0.5 [10, 8, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   48.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End compilation of model:  0.5 [10, 8, 4]\n",
      "Epoch 1/10\n",
      "594/594 [==============================] - 5s 9ms/step - loss: 0.0077\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0060\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 1s 2ms/step - loss: 0.0057\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 1s 2ms/step - loss: 0.0054\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 1s 2ms/step - loss: 0.0056\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 1s 2ms/step - loss: 0.0054\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 1s 2ms/step - loss: 0.0053\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 1s 2ms/step - loss: 0.0056\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 1s 2ms/step - loss: 0.0054\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 1s 2ms/step - loss: 0.0054\n",
      "148/148 [==============================] - 1s 4ms/step\n",
      "594/594 [==============================] - 0s 552us/step\n",
      "[CV]  dropout=0.5, neurons=[10, 8, 4], score=-0.005934676158987615, total=  24.4s\n",
      "[CV] dropout=0.5, neurons=[10, 8, 4] .................................\n",
      "Start model:  0.5 [10, 8, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End compilation of model:  0.5 [10, 8, 4]\n",
      "Epoch 1/10\n",
      "742/742 [==============================] - 5s 7ms/step - loss: 0.0076\n",
      "Epoch 2/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0059\n",
      "Epoch 3/10\n",
      "742/742 [==============================] - 2s 2ms/step - loss: 0.0056\n",
      "Epoch 4/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0056\n",
      "Epoch 5/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0056\n",
      "Epoch 6/10\n",
      "742/742 [==============================] - 2s 2ms/step - loss: 0.0054\n",
      "Epoch 7/10\n",
      "742/742 [==============================] - 2s 2ms/step - loss: 0.0055\n",
      "Epoch 8/10\n",
      "742/742 [==============================] - 2s 2ms/step - loss: 0.0054\n",
      "Epoch 9/10\n",
      "742/742 [==============================] - 2s 2ms/step - loss: 0.0055\n",
      "Epoch 10/10\n",
      "742/742 [==============================] - 2s 2ms/step - loss: 0.0054\n",
      "148/148 [==============================] - 1s 5ms/step\n",
      "742/742 [==============================] - 0s 576us/step\n",
      "[CV]  dropout=0.5, neurons=[10, 8, 4], score=-0.003853537448260279, total=  27.2s\n",
      "[CV] dropout=0.5, neurons=[20, 16, 8] ................................\n",
      "Start model:  0.5 [20, 16, 8]\n",
      "End compilation of model:  0.5 [20, 16, 8]\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 4s 24ms/step - loss: 0.0079\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0065\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0066\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0059\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0055\n",
      "148/148 [==============================] - 1s 5ms/step\n",
      "150/150 [==============================] - 0s 628us/step\n",
      "[CV]  dropout=0.5, neurons=[20, 16, 8], score=-0.007847366471872414, total=  12.6s\n",
      "[CV] dropout=0.5, neurons=[20, 16, 8] ................................\n",
      "Start model:  0.5 [20, 16, 8]\n",
      "End compilation of model:  0.5 [20, 16, 8]\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 5s 17ms/step - loss: 0.0095\n",
      "Epoch 2/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0087\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0079\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0074\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0070\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0074\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 1s 2ms/step - loss: 0.0071\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0070\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0066\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0067\n",
      "148/148 [==============================] - 1s 7ms/step\n",
      "298/298 [==============================] - 0s 723us/step\n",
      "[CV]  dropout=0.5, neurons=[20, 16, 8], score=-0.0034298872852633468, total=  17.6s\n",
      "[CV] dropout=0.5, neurons=[20, 16, 8] ................................\n",
      "Start model:  0.5 [20, 16, 8]\n",
      "End compilation of model:  0.5 [20, 16, 8]\n",
      "Epoch 1/10\n",
      "446/446 [==============================] - 5s 11ms/step - loss: 0.0078\n",
      "Epoch 2/10\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0066\n",
      "Epoch 3/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0064\n",
      "Epoch 4/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0064\n",
      "Epoch 5/10\n",
      "446/446 [==============================] - 1s 2ms/step - loss: 0.0058\n",
      "Epoch 6/10\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0058\n",
      "Epoch 7/10\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0059\n",
      "Epoch 8/10\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0055\n",
      "Epoch 9/10\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0058\n",
      "Epoch 10/10\n",
      "446/446 [==============================] - 1s 3ms/step - loss: 0.0057\n",
      "148/148 [==============================] - 1s 6ms/step\n",
      "446/446 [==============================] - 0s 618us/step\n",
      "[CV]  dropout=0.5, neurons=[20, 16, 8], score=-0.004626650079247869, total=  22.1s\n",
      "[CV] dropout=0.5, neurons=[20, 16, 8] ................................\n",
      "Start model:  0.5 [20, 16, 8]\n",
      "End compilation of model:  0.5 [20, 16, 8]\n",
      "Epoch 1/10\n",
      "594/594 [==============================] - 6s 10ms/step - loss: 0.0073\n",
      "Epoch 2/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0065\n",
      "Epoch 3/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0059\n",
      "Epoch 4/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0056\n",
      "Epoch 5/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0060\n",
      "Epoch 6/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0056\n",
      "Epoch 7/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0053\n",
      "Epoch 8/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0056\n",
      "Epoch 9/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0054\n",
      "Epoch 10/10\n",
      "594/594 [==============================] - 2s 3ms/step - loss: 0.0054\n",
      "148/148 [==============================] - 1s 8ms/step\n",
      "594/594 [==============================] - 0s 630us/step\n",
      "[CV]  dropout=0.5, neurons=[20, 16, 8], score=-0.0058777568432364004, total=  26.6s\n",
      "[CV] dropout=0.5, neurons=[20, 16, 8] ................................\n",
      "Start model:  0.5 [20, 16, 8]\n",
      "End compilation of model:  0.5 [20, 16, 8]\n",
      "Epoch 1/10\n",
      "742/742 [==============================] - 8s 11ms/step - loss: 0.0080\n",
      "Epoch 2/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0065\n",
      "Epoch 3/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0059\n",
      "Epoch 4/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0058\n",
      "Epoch 5/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0057\n",
      "Epoch 6/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0054\n",
      "Epoch 7/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0056\n",
      "Epoch 8/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0056\n",
      "Epoch 9/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0055\n",
      "Epoch 10/10\n",
      "742/742 [==============================] - 2s 3ms/step - loss: 0.0055\n",
      "148/148 [==============================] - 1s 8ms/step\n",
      "742/742 [==============================] - 1s 690us/step\n",
      "[CV]  dropout=0.5, neurons=[20, 16, 8], score=-0.003963014463315139, total=  33.6s\n",
      "[CV] dropout=0.5, neurons=[40, 32, 16] ...............................\n",
      "Start model:  0.5 [40, 32, 16]\n",
      "End compilation of model:  0.5 [40, 32, 16]\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 5s 36ms/step - loss: 0.0092\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0077\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0064\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0066\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0055\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0054\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0064\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0053\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0062\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 0.0059\n",
      "148/148 [==============================] - 1s 9ms/step\n",
      "150/150 [==============================] - ETA:  - 0s 702us/step\n",
      "[CV]  dropout=0.5, neurons=[40, 32, 16], score=-0.008134039415430429, total=  16.8s\n",
      "[CV] dropout=0.5, neurons=[40, 32, 16] ...............................\n",
      "Start model:  0.5 [40, 32, 16]\n",
      "End compilation of model:  0.5 [40, 32, 16]\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 7s 22ms/step - loss: 0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0085\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0082\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0073\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0075\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0069\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0069\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0072\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0069\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 1s 3ms/step - loss: 0.0071\n",
      "148/148 [==============================] - 1s 10ms/step\n",
      "298/298 [==============================] - 0s 656us/step\n",
      "[CV]  dropout=0.5, neurons=[40, 32, 16], score=-0.003826251479237013, total=  21.3s\n",
      "[CV] dropout=0.5, neurons=[40, 32, 16] ...............................\n",
      "Start model:  0.5 [40, 32, 16]\n",
      "End compilation of model:  0.5 [40, 32, 16]\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# create model\n",
    "model = KerasRegressor(build_fn=create_model, batch_size=4, epochs=10) \n",
    "##############################################################\n",
    "# grid search: dropout, neurons\n",
    "dropout = [0.5, 0.75]  # [0.25, 0.5]\n",
    "neurons = [[10, 8, 4], [20, 16, 8], [40, 32, 16]]\n",
    "param_space = dict(dropout=dropout, neurons=neurons)\n",
    "##############################################################\n",
    "tseries_cv = TimeSeriesSplit(n_splits=5).split(LSTM_training_inputs)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_space, n_jobs=1, cv=tseries_cv, verbose=5)\n",
    "\n",
    "print (\"Train input shape: \", LSTM_training_inputs.shape)\n",
    "print (\"Train output shape: \", LSTM_training_outputs.shape)\n",
    "\n",
    "grid_result = grid.fit(LSTM_training_inputs, LSTM_training_outputs)\n",
    "##############################################################\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "# ax1.plot(bt_history.epoch, bt_history.history['loss'])\n",
    "# ax1.set_title('Training Error')\n",
    "\n",
    "# if bt_model.loss == 'mae':\n",
    "#     ax1.set_ylabel('Mean Absolute Error (MAE)',fontsize=12)\n",
    "# elif bt_model.loss == 'mse':\n",
    "#     ax1.set_ylabel('Mean Squared Error (MSE)',fontsize=12)\n",
    "# else:\n",
    "#     ax1.set_ylabel('Model Loss',fontsize=12)\n",
    "# ax1.set_xlabel('# Epochs',fontsize=12)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = grid_result.best_estimator_\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "LSTM_test_outputs = training_set[target][window_len:].values\n",
    "ax1.set_xticks([datetime.date(2018,i+1,1) for i in range(12)])\n",
    "ax1.set_xticklabels([datetime.date(2018,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
    "ax1.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "         test_set[target][window_len:], label='Actual')\n",
    "ax1.plot(model_data[model_data['Date']>= split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "         (bt_model.predict(LSTM_test_inputs)), \n",
    "         label='Predicted')\n",
    "lstm_mse=np.mean(np.square(bt_model.predict(LSTM_test_inputs)-(test_set[target].values[window_len:])))\n",
    "ax1.annotate('MSE: %.4f'%lstm_mse, xy=(0.75, 0.9),  xycoords='axes fraction', xytext=(0.75, 0.9), textcoords='axes fraction')\n",
    "ax1.set_title('Test Set: Single Timepoint Prediction',fontsize=13)\n",
    "ax1.set_ylabel('Bitcoin Price change (%)',fontsize=12)\n",
    "ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "ax1.set_xticks([datetime.date(2018,i+1,1) for i in range(12)])\n",
    "ax1.set_xticklabels([datetime.date(2018,i+1,1).strftime('%b %d %Y')  for i in range(12)])\n",
    "ax1.plot(model_data[model_data['Date']< split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "         training_set[target][window_len:], label='Actual')\n",
    "ax1.plot(model_data[model_data['Date']< split_date]['Date'][window_len:].astype(datetime.datetime),\n",
    "         (bt_model.predict(LSTM_training_inputs)), \n",
    "         label='Predicted')\n",
    "ax1.annotate('MSE: %.4f'%np.mean(np.square(bt_model.predict(LSTM_training_inputs)-\\\n",
    "            (training_set[target].values[window_len:]))), \n",
    "             xy=(0.75, 0.9),  xycoords='axes fraction',\n",
    "            xytext=(0.75, 0.9), textcoords='axes fraction')\n",
    "ax1.set_title('Training Set: Single Timepoint Prediction',fontsize=13)\n",
    "ax1.set_ylabel('Bitcoin Price ($)',fontsize=12)\n",
    "ax1.legend(bbox_to_anchor=(0.1, 1), loc=2, borderaxespad=0., prop={'size': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_values = bt_model.predict(LSTM_training_inputs)\n",
    "max(pred_values)\n",
    "min(pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (bm1_mse)\n",
    "print (bm2_mse)\n",
    "print (lstm_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use loss as MSE to strongly penalize large errors\n",
    "def build_model(inputs, output_size, neurons, activ_func=\"linear\",\n",
    "                dropout=0.5, loss=\"mse\", optimizer=\"adam\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(neurons, return_sequences=True, input_shape=(inputs.shape[1], inputs.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    # model.add(LSTM(25, return_sequences=True))\n",
    "    # model.add(Dropout(dropout))\n",
    "    model.add(LSTM(2*neurons))\n",
    "    model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
